# -*- coding: utf-8 -*-
"""CISC 520 Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/163IoW9_uzPD_H3W3Cj2dCvizNMoE86XY
"""

pip install datasets

import torch
import torch.optim as optim
from torch.nn import BCEWithLogitsLoss
from torch.utils.data import DataLoader, TensorDataset
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
import openai
import streamlit as st

# Load the GoEmotions dataset
ds = load_dataset("google-research-datasets/go_emotions", "raw")

# Inspect the dataset
print(ds)

# Extract the text and label data
texts = ds['train']['text']

# Extract emotion columns from the dataset
emotion_columns = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',
                   'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',
                   'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',
                   'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',
                   'relief', 'remorse', 'sadness', 'surprise', 'neutral']

# Extract labels as a list of dictionaries
labels = ds['train'].to_pandas()[emotion_columns].values

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(emotion_columns))

# Split the dataset into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)

# Reduce the size of the training set to 10% for faster training
X_train_subset, _, y_train_subset, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)

# Tokenize the text data for the subset
train_encodings_subset = tokenizer(list(X_train_subset), padding=True, truncation=True, return_tensors='pt', max_length=512)

# Create TensorDataset and DataLoader for the subset
train_dataset_subset = TensorDataset(train_encodings_subset['input_ids'], train_encodings_subset['attention_mask'], torch.tensor(y_train_subset))
train_loader_subset = DataLoader(train_dataset_subset, batch_size=8, shuffle=True)

# Tokenize the test data (use the complete test set)
test_encodings = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt', max_length=512)

from torch.utils.data import DataLoader, TensorDataset

# Create TensorDataset for the training subset
train_dataset_subset = TensorDataset(train_encodings_subset['input_ids'], train_encodings_subset['attention_mask'], torch.tensor(y_train_subset))

# Create TensorDataset for the testing data
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(y_test))

# Create DataLoaders for the subset training data and the full testing data
train_loader_subset = DataLoader(train_dataset_subset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

import torch.optim as optim

# Set up the PyTorch AdamW optimizer
optimizer = optim.AdamW(model.parameters(), lr=2e-5)

# Set the device (GPU or CPU)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

from torch.nn import BCEWithLogitsLoss

# Set the model to training mode
model.train()

# Training loop parameters
epochs = 4  # Reduced to 2 epoch for faster results

for epoch in range(epochs):
    total_loss = 0
    for step, batch in enumerate(train_loader_subset):  # Use train_loader_subset here
        # Move batch to device (GPU/CPU)
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        labels = labels.float()

        # Clear previous gradients
        optimizer.zero_grad()

        # Forward pass: Get predictions and calculate loss
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits

        # Use BCEWithLogitsLoss for multi-label classification
        loss_function = BCEWithLogitsLoss()
        loss = loss_function(logits, labels.float())

        # Backward pass: Compute gradients and update weights
        loss.backward()
        optimizer.step()

        # Accumulate the training loss
        total_loss += loss.item()

        # Print progress every 10 batches
        if step % 10 == 0:
            print(f"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}")

    # Print the average loss for this epoch
    avg_train_loss = total_loss / len(train_loader_subset)
    print(f"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}")

from torch.utils.data import DataLoader

# Create a DataLoader for the test set with a smaller batch size
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(y_test))
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Set model to evaluation mode
model.eval()
predictions, true_labels = [], []

# Evaluate the model in smaller batches
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        # Get model predictions
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        batch_predictions = torch.sigmoid(logits) > 0.5

        # Store predictions and true labels
        predictions.extend(batch_predictions.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

# Calculate metrics after accumulating all predictions
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

accuracy = accuracy_score(true_labels, predictions)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='micro')

print(f"Test Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}")

# Print the average loss for this epoch
avg_train_loss = total_loss / len(train_loader_subset)
print(f"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}")

# Save the trained model
torch.save(model.state_dict(), "bert_emotion_model.pth")

from google.colab import files
files.download("bert_emotion_model.pth")